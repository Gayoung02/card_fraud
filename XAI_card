import os
import argparse
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    average_precision_score,
    precision_recall_curve,
    confusion_matrix,
    roc_curve
)
from sklearn.utils.class_weight import compute_class_weight
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.inspection import permutation_importance

# Optional SMOTE
try:
    from imblearn.over_sampling import SMOTE
    HAS_SMOTE = True
except Exception:
    HAS_SMOTE = False

# Optional SHAP
try:
    import shap
    HAS_SHAP = True
except Exception:
    HAS_SHAP = False

# Optional: XGBoost
try:
    from xgboost import XGBClassifier
    HAS_XGB = True
except Exception:
    HAS_XGB = False


# ---------------------------
# Utils
# ---------------------------
def print_banner(title: str):
    print("\n" + "=" * 80)
    print(title)
    print("=" * 80)


def ensure_numeric(df: pd.DataFrame) -> pd.DataFrame:
    """Keep only numeric columns."""
    numeric_df = df.select_dtypes(include=[np.number]).copy()
    dropped = set(df.columns) - set(numeric_df.columns)
    if dropped:
        print(f"[Info] Non-numeric columns dropped from features: {sorted(list(dropped))}")
    return numeric_df


def evaluate_classifier(y_true, y_prob, y_pred, title="Model"):
    """
    공통 평가 함수
    - ROC-AUC, PR-AUC 계산 및 출력
    - Confusion Matrix, Classification Report 출력
    - ROC / PR 곡선 PNG 저장
    - (roc_auc, pr_auc) 리턴
    """
    roc = roc_auc_score(y_true, y_prob)
    pr_auc = average_precision_score(y_true, y_prob)

    cm = confusion_matrix(y_true, y_pred)
    report = classification_report(y_true, y_pred, digits=4)

    print_banner(f"[{title}] Metrics")
    print(f"ROC-AUC       : {roc:.4f}")
    print(f"PR-AUC        : {pr_auc:.4f}")
    print("\nConfusion Matrix (TN, FP; FN, TP):\n", cm)
    print("\nClassification Report:\n", report)

    # Precision-Recall curve
    precision, recall, _ = precision_recall_curve(y_true, y_prob)
    plt.figure()
    plt.plot(recall, precision)
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(f"Precision-Recall Curve - {title}")
    plt.tight_layout()
    plt.savefig(f"pr_curve_{title.replace(' ', '_').lower()}.png")
    plt.close()

    # ROC curve
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    plt.figure()
    plt.plot(fpr, tpr)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve - {title}")
    plt.tight_layout()
    plt.savefig(f"roc_curve_{title.replace(' ', '_').lower()}.png")
    plt.close()

    return roc, pr_auc


def pick_threshold_by_precision(y_true, y_prob, min_precision=0.98):
    """
    Precision-Recall 커브에서
    Precision >= min_precision 를 만족하면서 Recall이 최대가 되는 threshold 선택.
    PR 커브: p, r 길이는 n+1, threshold는 n → p[:-1]/r[:-1]와 align.
    """
    p, r, thr = precision_recall_curve(y_true, y_prob)
    mask = p[:-1] >= min_precision
    if mask.any():
        idx = np.argmax(r[:-1][mask])
        return thr[mask][idx]
    return 0.5  # 목표 precision을 만족 못하면 기본값


def generate_prob_model_explanations(
    model_name,
    X_test,
    y_test,
    y_prob,
    y_pred,
    feature_names,
    threshold,
    shap_values=None,
    global_importances=None,
    top_k=5,
    file_name=None
):
    """
    확률 기반 이진 분류 모델(MLP, XGBoost 등)에 대해
    '사기(1)로 예측된 거래'마다 텍스트 설명을 만들어 파일로 저장.

    - shap_values: (n_samples, n_features) 또는 [class0, class1, ...] 리스트
    - global_importances: (n_features,) 형태의 전역 feature importance (permutation 등)
    """
    if file_name is None:
        file_name = f"{model_name.lower().replace(' ', '_')}_fraud_explanations.txt"

    fraud_indices = np.where(y_pred == 1)[0]

    lines = []
    lines.append(f"{model_name} 기반 사기 예측 거래 설명")
    lines.append("------------------------------------------------------------------")
    lines.append("※ 주의: 아래 feature 값들은 StandardScaler로 정규화(표준화)된 값 기준입니다.")
    lines.append("   - value: 해당 거래의 표준화된 feature 값(z-score)")
    lines.append("   - threshold: 분류에 사용된 사기 판정 임계값")
    if shap_values is not None:
        lines.append("   - shap: 해당 feature가 사기(양수) / 정상(음수) 판정에 기여한 정도")
    elif global_importances is not None:
        lines.append("   - global importance: 전체 데이터 기준으로 중요한 feature 순위")
    lines.append("")
    lines.append(f"사용된 분류 임계값(threshold): {threshold:.3f}")
    lines.append("")

    # shap_values 모양 정리
    sv = None
    if shap_values is not None:
        sv = shap_values
        if isinstance(sv, list):
            # binary일 때 [class0, class1] 구조일 수 있음
            if len(sv) >= 2:
                sv = sv[1]
            else:
                sv = sv[0]

    if len(fraud_indices) == 0:
        lines.append("테스트 데이터에서 사기(1)로 예측된 거래가 없습니다.")
    else:
        for idx in fraud_indices:
            x = X_test[idx]
            prob = y_prob[idx]
            true_label = y_test[idx]

            lines.append(f"[Case #{idx}]")
            lines.append(
                f"  - 실제 라벨(y_true) : {true_label} (0=정상, 1=사기)"
            )
            lines.append(
                f"  - 모델 예측(y_pred) : 1 (사기) "
                f"(임계값={threshold:.3f}, 예측 확률={prob:.4f})"
            )

            # feature 중요도 순서 계산
            if sv is not None:
                contrib = sv[idx]
                order = np.argsort(-np.abs(contrib))
                lines.append("  - 이 거래의 사기 판정에 가장 크게 기여한 feature (SHAP 기준):")
                for rank, j in enumerate(order[:top_k]):
                    fname = feature_names[j]
                    fval = x[j]
                    cval = contrib[j]
                    sign = "사기 방향 기여(+)" if cval > 0 else "정상 방향 기여(-)"
                    lines.append(
                        f"    * #{rank+1} {fname}: value={fval:.3f}, "
                        f"shap={cval:.4f} ({sign})"
                    )
            elif global_importances is not None:
                gi = np.asarray(global_importances)
                order = np.argsort(-np.abs(gi))
                lines.append("  - 전체 기준으로 중요한 feature 상위 항목과 해당 거래의 값:")
                for rank, j in enumerate(order[:top_k]):
                    fname = feature_names[j]
                    fval = x[j]
                    imp = gi[j]
                    lines.append(
                        f"    * #{rank+1} {fname}: value={fval:.3f}, "
                        f"global_importance={imp:.4f}"
                    )
            else:
                lines.append("  - (추가적인 feature 중요도 정보가 없어 상세 설명을 제공하지 못했습니다.)")

            lines.append("")

    with open(file_name, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print_banner(f"[{model_name}] 거래별 사기 예측 설명 저장 완료")
    print(f"Saved: {file_name}")


# ---------------------------
# Decision Tree (Val → Test)
# ---------------------------
def fit_decision_tree(
    X_train, y_train,
    X_val, y_val,
    X_test, y_test,
    max_depth=6,
    class_weight='balanced',
    feature_names=None,
    export_rules_depth=3,
    save_csv=True,
    threshold_precision=None
):
    """
    - Train: X_train, y_train
    - Threshold 선택: Validation (X_val, y_val)
    - 평가: Test (X_test, y_test)
    - 추가: 사기(1)로 예측된 건들에 대해 트리 경로를 텍스트로 설명 파일 생성
    """

    # class_weight가 dict이면 그대로 사용, 아니면 None
    cw = class_weight if isinstance(class_weight, dict) else None

    # 트리를 보수적으로 설정해서 과적합 및 FP 줄이기
    model = DecisionTreeClassifier(
        max_depth=max_depth,
        min_samples_split=200,   # 50 -> 200
        min_samples_leaf=100,    # 20 -> 100
        max_features="sqrt",
        random_state=42,
        class_weight=cw,
        criterion="gini"
    )
    model.fit(X_train, y_train)

    # Validation에서 threshold 선택
    y_val_prob = model.predict_proba(X_val)[:, 1]
    if threshold_precision is not None:
        cls_thr = pick_threshold_by_precision(
            y_val, y_val_prob, min_precision=threshold_precision
        )
        val_title = f"Decision Tree (VAL th@P>={threshold_precision:.2f} => {cls_thr:.3f})"
    else:
        cls_thr = 0.5
        val_title = "Decision Tree (VAL, default thr=0.5)"

    # 필요하면 Validation 성능 확인할 때 사용
    # y_val_pred = (y_val_prob >= cls_thr).astype(int)
    # evaluate_classifier(y_val, y_val_prob, y_val_pred, title=val_title)

    # Test 평가
    y_test_prob = model.predict_proba(X_test)[:, 1]
    y_test_pred = (y_test_prob >= cls_thr).astype(int)
    test_title = f"Decision Tree (TEST, thr={cls_thr:.3f})"

    # Feature importance
    importances = model.feature_importances_
    fi = pd.DataFrame({"feature": feature_names, "importance": importances}) \
           .sort_values("importance", ascending=False)
    print_banner("[DecisionTree] Top feature importances")
    print(fi.head(20).to_string(index=False))
    if save_csv:
        fi.to_csv("dt_feature_importances.csv", index=False)

    # Tree preview image
    try:
        plt.figure(figsize=(14, 8))
        plot_tree(
            model,
            feature_names=feature_names,
            filled=True,
            max_depth=export_rules_depth,
            rounded=True,
            fontsize=8
        )
        plt.title(f"Decision Tree (depth≤{export_rules_depth} view)")
        plt.tight_layout()
        plt.savefig("tree_preview.png")
        plt.close()
    except Exception as e:
        print(f"[Warn] Tree plot failed: {e}")

    # Rule text export (전체 규칙 요약)
    try:
        rules = export_text(model, feature_names=feature_names, max_depth=export_rules_depth)
        with open("decision_tree_rules_depth3.txt", "w", encoding="utf-8") as f:
            f.write(rules)
        print_banner("[DecisionTree] Rules (depth<=3)")
        print(rules)
    except Exception as e:
        print(f"[Warn] Rule export failed: {e}")

    # 공통 성능 평가
    roc, pr_auc = evaluate_classifier(y_test, y_test_prob, y_test_pred, title=test_title)

    # ============================
    # 거래별 설명 텍스트 생성 (Decision Tree)
    # ============================
    try:
        tree = model.tree_
        explanations = []

        fraud_indices = np.where(y_test_pred == 1)[0]

        header = []
        header.append("Decision Tree 기반 사기 예측 거래 설명")
        header.append("------------------------------------------------------------------")
        header.append("※ 주의: 아래 조건의 feature 값과 임계값(threshold)은 StandardScaler로")
        header.append("   정규화(표준화)된 값 기준입니다. (원 데이터가 아니라 z-score 기준)")
        header.append("   - value: 해당 거래의 표준화된 feature 값")
        header.append("   - threshold: 트리가 분기한 표준화된 기준 값")
        header.append("")
        explanations.append("\n".join(header))

        if len(fraud_indices) == 0:
            explanations.append("테스트 데이터에서 사기(1)로 예측된 거래가 없습니다.")
        else:
            for idx in fraud_indices:
                x = X_test[idx]
                prob = y_test_prob[idx]
                true_label = y_test[idx]

                node_id = 0
                path_lines = []

                # 리프에 도달할 때까지 분기 경로 추적
                while tree.feature[node_id] != -2:  # -2면 leaf
                    feat_idx = tree.feature[node_id]
                    node_thr = tree.threshold[node_id]
                    feat_name = feature_names[feat_idx] if feature_names is not None else f"feature_{feat_idx}"
                    x_val = x[feat_idx]

                    if x_val <= node_thr:
                        decision = "<="
                        next_node = tree.children_left[node_id]
                    else:
                        decision = ">"
                        next_node = tree.children_right[node_id]

                    path_lines.append(
                        f"  - {feat_name}: value={x_val:.3f} {decision} threshold={node_thr:.3f}"
                    )
                    node_id = next_node

                # 리프 노드에서 클래스 분포 확인
                node_samples = tree.n_node_samples[node_id]
                value = tree.value[node_id][0]  # [n_normal, n_fraud]
                normal_count, fraud_count = value[0], value[1]
                leaf_total = value.sum()
                leaf_fraud_ratio = fraud_count / leaf_total if leaf_total > 0 else float("nan")

                case_lines = []
                case_lines.append(f"[Case #{idx}]")
                case_lines.append(
                    f"  - 실제 라벨(y_true) : {true_label} "
                    f"(0=정상, 1=사기)"
                )
                case_lines.append(
                    f"  - 모델 예측(y_pred) : 1 (사기) "
                    f"(분류 임계값={cls_thr:.3f}, 예측 확률={prob:.4f})"
                )
                case_lines.append(
                    f"  - 이 리프 노드의 데이터 분포: 정상={normal_count:.0f}, 사기={fraud_count:.0f}, "
                    f"사기 비율={leaf_fraud_ratio:.4f}, 노드 샘플 수={node_samples}"
                )
                case_lines.append("  - 이 거래가 사기로 분류된 의사결정 경로:")
                case_lines.extend(path_lines)
                case_lines.append("")  # 공백 줄

                explanations.append("\n".join(case_lines))

        with open("decision_tree_fraud_explanations.txt", "w", encoding="utf-8") as f:
            f.write("\n\n".join(explanations))

        print_banner("[DecisionTree] 거래별 사기 예측 설명 저장 완료")
        print("Saved: decision_tree_fraud_explanations.txt")
    except Exception as e:
        print(f"[Warn] Failed to create per-transaction explanations: {e}")

    return model, fi, cls_thr, roc, pr_auc


# ---------------------------
# MLP (Val → Test)
# ---------------------------
def fit_mlp(
    X_train, y_train,
    X_val, y_val,
    X_test, y_test,
    feature_names,
    hidden=(64, 32, 16),
    use_smote=False,
    threshold_precision=None
):
    from sklearn.neural_network import MLPClassifier

    X_tr, y_tr = X_train, y_train
    if use_smote and HAS_SMOTE:
        print("[MLP] Applying SMOTE on training set...")
        sm = SMOTE(random_state=42, sampling_strategy="auto")
        X_tr, y_tr = sm.fit_resample(X_train, y_train)

    mlp = MLPClassifier(
        hidden_layer_sizes=hidden,
        activation="relu",
        solver="adam",
        learning_rate_init=1e-3,
        alpha=1e-4,             # L2 정규화
        max_iter=400,           # 충분한 학습
        random_state=42,
        early_stopping=True,
        n_iter_no_change=20,
        validation_fraction=0.15
    )
    mlp.fit(X_tr, y_tr)

    # Validation에서 threshold 선택
    y_val_prob = mlp.predict_proba(X_val)[:, 1]
    if threshold_precision is not None:
        thr = pick_threshold_by_precision(y_val, y_val_prob, min_precision=threshold_precision)
        val_title = f"MLP (VAL th@P>={threshold_precision:.2f} => {thr:.3f})"
    else:
        thr = 0.5
        val_title = "MLP (VAL, default thr=0.5)"

    # y_val_pred = (y_val_prob >= thr).astype(int)
    # evaluate_classifier(y_val, y_val_prob, y_val_pred, title=val_title)

    # Test 평가
    y_test_prob = mlp.predict_proba(X_test)[:, 1]
    y_test_pred = (y_test_prob >= thr).astype(int)
    test_title = f"MLP (TEST, thr={thr:.3f})"
    roc, pr_auc = evaluate_classifier(y_test, y_test_prob, y_test_pred, title=test_title)

    # XAI: permutation importance (전역 중요도)
    print_banner("[MLP] Permutation importance (global)")
    r = permutation_importance(
        mlp, X_test, y_test,
        n_repeats=5,
        random_state=42,
        scoring='average_precision'
    )
    pi = pd.DataFrame(
        {'feature': feature_names, 'importance': r.importances_mean}
    ).sort_values('importance', ascending=False)
    pi.to_csv("mlp_permutation_importance.csv", index=False)
    print(pi.head(20).to_string(index=False))
    print("Saved: mlp_permutation_importance.csv")

    # SHAP KernelExplainer (선택적, 시각화용)
    if HAS_SHAP:
        try:
            print_banner("[MLP] SHAP (KernelExplainer, sampled)")
            rng = np.random.RandomState(42)
            bg_idx = rng.choice(len(X_train), size=min(300, len(X_train)), replace=False)
            xs_idx = rng.choice(len(X_test),  size=min(300, len(X_test)),  replace=False)
            bg = X_train[bg_idx]
            xs = X_test[xs_idx]

            f = lambda data: mlp.predict_proba(data)[:, 1]
            explainer = shap.KernelExplainer(f, bg)
            shap_values_sample = explainer.shap_values(xs, nsamples=200)

            shap.summary_plot(shap_values_sample, xs, feature_names=feature_names, show=False)
            plt.title("MLP SHAP (KernelExplainer, sample)")
            plt.tight_layout()
            plt.savefig("mlp_shap_kernel.png")
            plt.close()
            print("[MLP] SHAP plot saved: mlp_shap_kernel.png")
        except Exception as e:
            print(f"[Warn] SHAP KernelExplainer failed: {e}")

    # 거래별 설명 텍스트 생성 (MLP)
    try:
        generate_prob_model_explanations(
            model_name="MLP",
            X_test=X_test,
            y_test=y_test,
            y_prob=y_test_prob,
            y_pred=y_test_pred,
            feature_names=feature_names,
            threshold=thr,
            shap_values=None,  # 여기서는 SHAP 대신 전역 중요도 사용
            global_importances=r.importances_mean,
            top_k=5,
            file_name="mlp_fraud_explanations.txt"
        )
    except Exception as e:
        print(f"[Warn] Failed to create MLP per-transaction explanations: {e}")

    return mlp, thr, roc, pr_auc


# ---------------------------
# XGBoost (Val → Test)
# ---------------------------
def fit_xgboost(
    X_train, y_train,
    X_val, y_val,
    X_test, y_test,
    feature_names,
    threshold_precision=None,
    save_csv=True
):
    if not HAS_XGB:
        print("[XGBoost] XGBoost not installed. Skipping.")
        return None, None, None, None

    # class imbalance 대응
    pos = (y_train == 1).sum()
    neg = (y_train == 0).sum()
    scale_pos_weight = neg / max(pos, 1)
    print(f"[XGBoost] Using scale_pos_weight: {scale_pos_weight:.2f}")

    model = XGBClassifier(
        random_state=42,
        n_estimators=200,
        max_depth=5,
        learning_rate=0.05,
        scale_pos_weight=scale_pos_weight,
        eval_metric="aucpr",
        early_stopping_rounds=20,
        n_jobs=-1
    )

    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=False
    )

    # Validation에서 threshold 선택
    y_val_prob = model.predict_proba(X_val)[:, 1]
    if threshold_precision is not None:
        thr = pick_threshold_by_precision(y_val, y_val_prob, min_precision=threshold_precision)
        val_title = f"XGBoost (VAL th@P>={threshold_precision:.2f} => {thr:.3f})"
    else:
        thr = 0.5
        val_title = "XGBoost (VAL, default thr=0.5)"

    # y_val_pred = (y_val_prob >= thr).astype(int)
    # evaluate_classifier(y_val, y_val_prob, y_val_pred, title=val_title)

    # Test 평가
    y_test_prob = model.predict_proba(X_test)[:, 1]
    y_test_pred = (y_test_prob >= thr).astype(int)
    test_title = f"XGBoost (TEST, thr={thr:.3f})"
    roc, pr_auc = evaluate_classifier(y_test, y_test_prob, y_test_pred, title=test_title)

    # Feature importance
    importances = model.feature_importances_
    fi = pd.DataFrame({"feature": feature_names, "importance": importances}) \
           .sort_values("importance", ascending=False)
    print_banner("[XGBoost] Top feature importances")
    print(fi.head(20).to_string(index=False))
    if save_csv:
        fi.to_csv("xgb_feature_importances.csv", index=False)

    # SHAP TreeExplainer (가능하면 개별 거래 설명에도 활용)
    shap_values_full = None
    if HAS_SHAP:
        try:
            print_banner("[XGBoost] SHAP (TreeExplainer)")
            explainer = shap.TreeExplainer(model)
            shap_values_full = explainer.shap_values(X_test)

            shap.summary_plot(shap_values_full, X_test, feature_names=feature_names, show=False)
            plt.title("XGBoost SHAP (TreeExplainer)")
            plt.tight_layout()
            plt.savefig("xgb_shap_summary.png")
            plt.close()
            print("[XGBoost] SHAP plot saved: xgb_shap_summary.png")
        except Exception as e:
            print(f"[Warn] XGB SHAP TreeExplainer failed: {e}")
            shap_values_full = None

    # 거래별 설명 텍스트 생성 (XGBoost)
    try:
        generate_prob_model_explanations(
            model_name="XGBoost",
            X_test=X_test,
            y_test=y_test,
            y_prob=y_test_prob,
            y_pred=y_test_pred,
            feature_names=feature_names,
            threshold=thr,
            shap_values=shap_values_full,
            global_importances=importances,
            top_k=5,
            file_name="xgb_fraud_explanations.txt"
        )
    except Exception as e:
        print(f"[Warn] Failed to create XGBoost per-transaction explanations: {e}")

    return model, thr, roc, pr_auc


# ---------------------------
# Main
# ---------------------------
def main(args):
    # Load
    print_banner("Load dataset")
    try:
        df = pd.read_csv(args.data_path, on_bad_lines='skip')
        print(df.head())
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return

    # Expect 'fraud' column
    if "fraud" not in df.columns:
        raise ValueError("Label column 'fraud' not found. Please ensure your CSV has 'fraud' column.")

    # Label 처리
    y_raw = df["fraud"]
    if y_raw.isnull().any() or np.isinf(y_raw).any():
        print("[Info] Handling non-finite values in 'fraud' column...")
        y_raw = y_raw.replace([np.inf, -np.inf], np.nan).fillna(0)

    y = y_raw.astype(int).values

    # Features: numeric only
    X_raw = df.drop(columns=["fraud"])
    X_raw = ensure_numeric(X_raw)
    X_raw.replace([np.inf, -np.inf], np.nan, inplace=True)
    feature_names = list(X_raw.columns)

    print_banner("Basic info")
    print(f"Shape: X={X_raw.shape}, y={y.shape}")
    pos = (y == 1).sum()
    neg = (y == 0).sum()
    print(f"Label distribution -> normal(0): {neg}, fraud(1): {pos}, ratio(1): {pos / max(1, len(y)):.6f}")

    # ---------------------------
    # Train / Val / Test split (60 / 20 / 20)
    # ---------------------------
    # 1) Train_temp / Test (전체의 20%를 Test로 먼저 분리)
    X_train_temp_raw, X_test_raw, y_train_temp, y_test = train_test_split(
        X_raw.values, y,
        test_size=0.2,          # 20% → Test
        random_state=42,
        stratify=y
    )

    # 2) Train / Val (남은 80%를 75:25로 나눔 → 전체 기준 60:20)
    X_train_raw, X_val_raw, y_train, y_val = train_test_split(
        X_train_temp_raw, y_train_temp,
        test_size=0.25,         # 0.8 * 0.25 = 0.2 → Val
        random_state=42,
        stratify=y_train_temp
    )

    # Imputer (fit on train만)
    imputer = SimpleImputer(strategy='mean')
    X_train_imp = imputer.fit_transform(X_train_raw)
    X_val_imp = imputer.transform(X_val_raw)
    X_test_imp = imputer.transform(X_test_raw)

    # Scaler (fit on train만)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train_imp)
    X_val = scaler.transform(X_val_imp)
    X_test = scaler.transform(X_test_imp)

    # Class weight (for DecisionTree)
    classes = np.array([0, 1])
    cw = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
    class_weight = {0: cw[0], 1: cw[1]}
    print_banner("Computed class_weight (train only)")
    print(class_weight)

    results = []  # 모델별 AUC 요약용

    # 1) Decision Tree
    dt_model, dt_fi, dt_thr, dt_roc, dt_pr = fit_decision_tree(
        X_train, y_train,
        X_val, y_val,
        X_test, y_test,
        max_depth=args.dt_max_depth,
        class_weight=class_weight,
        feature_names=feature_names,
        export_rules_depth=3,
        save_csv=True,
        threshold_precision=args.threshold_precision
    )
    results.append({
        "model": "Decision Tree",
        "roc_auc": dt_roc,
        "pr_auc": dt_pr
    })

    # 2) MLP
    mlp_model, mlp_thr, mlp_roc, mlp_pr = fit_mlp(
        X_train, y_train,
        X_val, y_val,
        X_test, y_test,
        feature_names=feature_names,
        hidden=tuple(args.mlp_hidden),
        use_smote=args.use_smote,
        threshold_precision=args.threshold_precision
    )
    results.append({
        "model": "MLP",
        "roc_auc": mlp_roc,
        "pr_auc": mlp_pr
    })

    # 3) XGBoost
    xgb_model, xgb_thr, xgb_roc, xgb_pr = fit_xgboost(
        X_train, y_train,
        X_val, y_val,
        X_test, y_test,
        feature_names=feature_names,
        threshold_precision=args.threshold_precision
    )
    if xgb_model is not None:
        results.append({
            "model": "XGBoost",
            "roc_auc": xgb_roc,
            "pr_auc": xgb_pr
        })

    # ---------------------------
    # 모델별 AUC 요약
    # ---------------------------
    if results:
        print_banner("Summary: Area under curves (performance)")
        df_res = pd.DataFrame(results)
        print(df_res.to_string(index=False))
        df_res.to_csv("model_auc_summary.csv", index=False)
        print("\n[Info] Saved summary to model_auc_summary.csv")

    # ---------------------------
    # Preprocessor & 모델 저장
    # ---------------------------
    print_banner("Saving models and preprocessors")
    try:
        joblib.dump(scaler, 'scaler.joblib')
        joblib.dump(imputer, 'imputer.joblib')
        print("Saved: scaler.joblib, imputer.joblib")

        # 최종 배포용 모델: XGBoost 있으면 XGB, 없으면 Decision Tree
        if xgb_model is not None:
            joblib.dump(xgb_model, 'xgb_model.joblib')
            print("Saved: xgb_model.joblib (Recommended model)")
        else:
            joblib.dump(dt_model, 'dt_model.joblib')
            print("Saved: dt_model.joblib (Fallback model)")
    except Exception as e:
        print(f"[Warn] Failed to save models: {e}")

    print_banner("Done")
    print("Saved figures: pr_curve_*.png, roc_curve_*.png, tree_preview.png, "
          "decision_tree_rules_depth3.txt, "
          "mlp_shap_kernel.png (if SHAP), mlp_permutation_importance.csv, "
          "xgb_shap_summary.png (if SHAP), xgb_feature_importances.csv (if XGB), "
          "dt_feature_importances.csv, "
          "model_auc_summary.csv, "
          "decision_tree_fraud_explanations.txt, "
          "mlp_fraud_explanations.txt, "
          "xgb_fraud_explanations.txt.")
    print("Saved models: scaler.joblib, imputer.joblib, "
          "xgb_model.joblib (if XGB) or dt_model.joblib (fallback).")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, default="card_transdata.csv",
                        help="Path to card_transdata.csv")
    parser.add_argument("--dt_max_depth", type=int, default=6,
                        help="DecisionTree max depth")
    parser.add_argument("--mlp_hidden", type=int, nargs="+", default=[64, 32, 16],
                        help="MLP hidden layer sizes, e.g., --mlp_hidden 64 32 16")
    parser.add_argument("--use_smote", action="store_true",
                        help="Apply SMOTE to MLP training set (requires imblearn)")
    parser.add_argument("--threshold_precision", type=float, default=0.98,
                        help="Precision 기준으로 threshold 선택 (e.g., 0.98)")
    args, _ = parser.parse_known_args()
    main(args)
